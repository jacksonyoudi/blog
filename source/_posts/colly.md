---
title: goçˆ¬è™«æ¡†æ¶collyæºç ä»¥åŠè½¯ä»¶æ¶æ„åˆ†æ
date: 2021-08-01 21:54:55
img: https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/RYdTI7.png
top: true
hide: false
cover: true
coverImg: https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/pL4vEU.jpg
toc: true
mathjax: false
summary: ä»‹ç»goçˆ¬è™«æ¡†æ¶ï¼Œä»æºç ä»‹ç»collyæ•°æ®æµè½¬è¿‡ç¨‹
keywords:
-  go
- colly
- çˆ¬è™«
- crawler

categories: æŠ€æœ¯
tags:
-  go
-  colly
- çˆ¬è™«
- crawler


---


![](https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/6hvTNM.jpg)

æ— æ„ä¸­å‘ç°äº†`colly`,æˆ‘ä¸€ç›´æ˜¯ä½¿ç”¨pythonè¿›è¡Œçˆ¬è™«çš„ï¼Œ å­¦ä¹ golangçš„ä½¿ç”¨ï¼Œ ç”¨`go`å‚è€ƒ`scrapy`æ¶æ„å†™äº†ä¸€ä¸ªçˆ¬è™«çš„æ¡†æ¶demoã€‚æˆ‘ä¸€ç›´ä»¥ä¸ºgoä¸é€‚åˆåšçˆ¬è™«ï¼Œ goçš„é¢†åŸŸæ˜¯åç«¯æœåŠ¡ã€‚ç„¶åå»æœç´¢äº†ä¸€ä¸‹`colly`, å‘ç°è¿˜æ˜¯å¾ˆæµè¡Œã€‚ æˆ‘ä¸ªäººè¿˜æ˜¯æ¯”è¾ƒå–œæ¬¢çˆ¬è™«ï¼Œ ç½‘ç»œä¸Šçš„æ•°æ®å°±æ˜¯å…¬å¼€çš„APIï¼Œ æ‰€ä»¥ï¼Œ çˆ¬è™«å»è¯·æ±‚æ¥å£è·å–æ•°æ®ã€‚å½“ç„¶æˆ‘æ˜¯éµå¾ªå›å­åè®®çš„ã€‚

å¥½ï¼Œ ä¸‹é¢è¿›å…¥æ­£é¢˜ï¼Œä»‹ç»`colly`

### collyä»‹ç»
`Lightning Fast and Elegant Scraping Framework for Gophers`

`Colly provides a clean interface to write any kind of crawler/scraper/spider.`
å®˜æ–¹çš„ä»‹ç»ï¼Œgocollyå¿«é€Ÿä¼˜é›…ï¼Œåœ¨å•æ ¸ä¸Šæ¯ç§’å¯ä»¥å‘èµ·1Kä»¥ä¸Šè¯·æ±‚ï¼›ä»¥å›è°ƒå‡½æ•°çš„å½¢å¼æä¾›äº†ä¸€ç»„æ¥å£ï¼Œå¯ä»¥å®ç°ä»»æ„ç±»å‹çš„çˆ¬è™«ï¼›ä¾èµ–goqueryåº“å¯ä»¥åƒjqueryä¸€æ ·é€‰æ‹©webå…ƒç´ ã€‚


### å®‰è£…ä½¿ç”¨
[colly](https://github.com/gocolly/colly)
[å®˜ç½‘](http://go-colly.org/)

```
go get -u github.com/gocolly/colly/...
```

```go
import "github.com/gocolly/colly"
```



### æ¶æ„ç‰¹ç‚¹
äº†è§£çˆ¬è™«çš„éƒ½çŸ¥é“ä¸€ä¸ªçˆ¬è™«è¯·æ±‚çš„ç”Ÿå‘½å‘¨æœŸ

>1. æ„å»ºè¯·æ±‚
>2. å‘é€è¯·æ±‚
>3. è·å–æ–‡æ¡£æˆ–æ•°æ®
>4. è§£ææ–‡æ¡£æˆ–æ¸…æ´—æ•°æ®
>5. æ•°æ®å¤„ç†æˆ–æŒä¹…åŒ–


scrapyçš„è®¾è®¡ç†å¿µæ˜¯å°†ä¸Šé¢çš„æ¯ä¸€ä¸ªæ­¥éª¤æŠ½ç¦»å‡ºæ¥ï¼Œç„¶ååšå‡ºç»„ä»¶çš„å½¢å¼ï¼Œ æœ€åé€šè¿‡è°ƒåº¦ç»„æˆæµæ°´çº¿çš„å·¥ä½œå½¢å¼ã€‚
æˆ‘ä»¬çœ‹ä¸€ä¸‹scrapyçš„æ¶æ„å›¾ï¼Œ è¿™é‡Œåªæ˜¯ç®€å•çš„ä»‹ç»ä¸‹ï¼Œ åé¢æœ‰æ—¶é—´ï¼Œæˆ‘æ·±å…¥ä»‹ç»scrapy
![0kfy9f](https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/0kfy9f.jpg)


å¦‚å›¾ï¼Œ`downloader`è´Ÿè´£è¯·æ±‚è·å–é¡µé¢ï¼Œ`spiders`ä¸­å†™å…·ä½“è§£ææ–‡æ¡£çš„é€»è¾‘ï¼Œ`item PipeLine`æ•°æ®æœ€åå¤„ç†ï¼Œ ä¸­é—´æœ‰ä¸€äº›ä¸­é—´ä»¶ï¼Œå¯ä»¥ä¸€äº›åŠŸèƒ½çš„è£…é¥°ã€‚æ¯”å¦‚ï¼Œä»£ç†ï¼Œè¯·æ±‚é¢‘ç‡ç­‰ã€‚


æˆ‘ä»¬ä»‹ç»ä¸€ä¸‹collyçš„æ¶æ„ç‰¹ç‚¹
collyçš„é€»è¾‘æ›´åƒæ˜¯é¢å‘è¿‡ç¨‹ç¼–ç¨‹çš„ï¼Œ collyçš„é€»è¾‘å°±æ˜¯æŒ‰ä¸Šé¢ç”Ÿå‘½å‘¨æœŸçš„é¡ºåºç®¡é“å¤„ç†ï¼Œ åªæ˜¯åœ¨ä¸åŒé˜¶æ®µï¼ŒåŠ ä¸Šå›è°ƒå‡½æ•°è¿›è¡Œè¿‡æ»¤çš„æ—¶å€™è¿›è¡Œå¤„ç†ã€‚
![](https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/IS4YSc.jpg)





ä¸‹é¢ä¹ŸæŒ‰ç…§è¿™ä¸ªé€»è¾‘è¿›è¡Œä»‹ç»


### æºç åˆ†æ

å…ˆç»™ä¸€ä¸ªğŸŒ°
```go

package main

import (
	"fmt"

	"github.com/gocolly/colly"
)

func main() {
	// Instantiate default collector
	c := colly.NewCollector(
		// Visit only domains: hackerspaces.org, wiki.hackerspaces.org
		colly.AllowedDomains("hackerspaces.org", "wiki.hackerspaces.org"),
	)

	// On every a element which has href attribute call callback
	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Attr("href")
		// Print link
		fmt.Printf("Link found: %q -> %s\n", e.Text, link)
		// Visit link found on page
		// Only those links are visited which are in AllowedDomains
		c.Visit(e.Request.AbsoluteURL(link))
	})

	// Before making a request print "Visiting ..."
	c.OnRequest(func(r *colly.Request) {
		fmt.Println("Visiting", r.URL.String())
	})

	// Start scraping on https://hackerspaces.org
	c.Visit("https://hackerspaces.org/")
}

```
è¿™æ˜¯å®˜æ–¹ç»™çš„ç¤ºä¾‹ï¼Œ å¯ä»¥çœ‹åˆ°`colly.NewCollector`åˆ›å»ºä¸€ä¸ª`æ”¶é›†å™¨`ï¼Œ collyçš„æ‰€æœ‰å¤„ç†é€»è¾‘éƒ½æ˜¯ä»¥`Collector`ä¸ºæ ¸å¿ƒè¿›è¡Œæ“ä½œçš„ã€‚


æˆ‘ä»¬çœ‹ä¸€ä¸‹ `Collector`ç»“æ„ä½“çš„å®šä¹‰
```go
// Collector provides the scraper instance for a scraping job
type Collector struct {
	// UserAgent is the User-Agent string used by HTTP requests
	UserAgent string
	// MaxDepth limits the recursion depth of visited URLs.
	// Set it to 0 for infinite recursion (default).
	MaxDepth int
	// AllowedDomains is a domain whitelist.
	// Leave it blank to allow any domains to be visited
	AllowedDomains []string
	// DisallowedDomains is a domain blacklist.
	DisallowedDomains []string
	// DisallowedURLFilters is a list of regular expressions which restricts
	// visiting URLs. If any of the rules matches to a URL the
	// request will be stopped. DisallowedURLFilters will
	// be evaluated before URLFilters
	// Leave it blank to allow any URLs to be visited
	DisallowedURLFilters []*regexp.Regexp
	// URLFilters is a list of regular expressions which restricts
	// visiting URLs. If any of the rules matches to a URL the
	// request won't be stopped. DisallowedURLFilters will
	// be evaluated before URLFilters

	// Leave it blank to allow any URLs to be visited
	URLFilters []*regexp.Regexp

	// AllowURLRevisit allows multiple downloads of the same URL
	AllowURLRevisit bool
	// MaxBodySize is the limit of the retrieved response body in bytes.
	// 0 means unlimited.
	// The default value for MaxBodySize is 10MB (10 * 1024 * 1024 bytes).
	MaxBodySize int
	// CacheDir specifies a location where GET requests are cached as files.
	// When it's not defined, caching is disabled.
	CacheDir string
	// IgnoreRobotsTxt allows the Collector to ignore any restrictions set by
	// the target host's robots.txt file.  See http://www.robotstxt.org/ for more
	// information.
	IgnoreRobotsTxt bool
	// Async turns on asynchronous network communication. Use Collector.Wait() to
	// be sure all requests have been finished.
	Async bool
	// ParseHTTPErrorResponse allows parsing HTTP responses with non 2xx status codes.
	// By default, Colly parses only successful HTTP responses. Set ParseHTTPErrorResponse
	// to true to enable it.
	ParseHTTPErrorResponse bool
	// ID is the unique identifier of a collector
	ID uint32
	// DetectCharset can enable character encoding detection for non-utf8 response bodies
	// without explicit charset declaration. This feature uses https://github.com/saintfish/chardet
	DetectCharset bool
	// RedirectHandler allows control on how a redirect will be managed
	RedirectHandler func(req *http.Request, via []*http.Request) error
	// CheckHead performs a HEAD request before every GET to pre-validate the response
	CheckHead         bool
	store             storage.Storage
	debugger          debug.Debugger
	robotsMap         map[string]*robotstxt.RobotsData
	htmlCallbacks     []*htmlCallbackContainer
	xmlCallbacks      []*xmlCallbackContainer
	requestCallbacks  []RequestCallback
	responseCallbacks []ResponseCallback
	errorCallbacks    []ErrorCallback
	scrapedCallbacks  []ScrapedCallback
	requestCount      uint32
	responseCount     uint32
	backend           *httpBackend
	wg                *sync.WaitGroup
	lock              *sync.RWMutex
}
```

ä¸Šé¢çš„å…·ä½“å±æ€§æˆ‘å°±ä¸ä»‹ç»äº†ï¼Œ çœ‹çœ‹æ³¨é‡Šä¹Ÿå°±æ‡‚äº†ã€‚
æˆ‘å°±å…ˆæŒ‰ä¸Šé¢çš„ç¤ºä¾‹è§£é‡Šæºç 
```bash
    // åˆ›å»ºä¸€ä¸ª Collectorå¯¹è±¡
	c := colly.NewCollector(
		// Visit only domains: hackerspaces.org, wiki.hackerspaces.org
		colly.AllowedDomains("hackerspaces.org", "wiki.hackerspaces.org"),
	)

	// æ·»åŠ ä¸€ä¸ªHTMLçš„å›è°ƒå‡½æ•°
	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Attr("href")
		// Print link
		fmt.Printf("Link found: %q -> %s\n", e.Text, link)
		// Visit link found on page
		// Only those links are visited which are in AllowedDomains
		c.Visit(e.Request.AbsoluteURL(link))
	})

	// æ·»åŠ ä¸€ä¸ª Requsetå›è°ƒå‡½æ•°
	c.OnRequest(func(r *colly.Request) {
		fmt.Println("Visiting", r.URL.String())
	})

	// å¼€å§‹çˆ¬å–
	c.Visit("https://hackerspaces.org/")
```
å›è°ƒå‡½æ•°å¦‚ä½•ç”¨ï¼Ÿ ä»€ä¹ˆä½œç”¨ï¼Ÿ å…ˆå–ä¸ªå…³å­ï¼Œ `c.Visit("https://hackerspaces.org/")`æ˜¯å…¥å£ï¼Œ é‚£å°±å…ˆåˆ†æå®ƒï¼Œ

```go
// Visit starts Collector's collecting job by creating a
// request to the URL specified in parameter.
// Visit also calls the previously provided callbacks
func (c *Collector) Visit(URL string) error {
	if c.CheckHead {
		if check := c.scrape(URL, "HEAD", 1, nil, nil, nil, true); check != nil {
			return check
		}
	}
	return c.scrape(URL, "GET", 1, nil, nil, nil, true)
}
```
ğŸ‘†åˆå‡ºæ¥ä¸€ä¸ªæ–°çš„methodï¼Œ

```go
func (c *Collector) scrape(u, method string, depth int, requestData io.Reader, ctx *Context, hdr http.Header, checkRevisit bool) error {
    // æ£€æŸ¥è¯·æ±‚æ˜¯å¦åˆæ³•
	if err := c.requestCheck(u, method, depth, checkRevisit); err != nil {
		return err
	}
    // è§£æurlï¼Œ
	parsedURL, err := url.Parse(u)
	if err != nil {
		return err
	}
	if parsedURL.Scheme == "" {
		parsedURL.Scheme = "http"
	}
	if !c.isDomainAllowed(parsedURL.Hostname()) {
		return ErrForbiddenDomain
	}
    // robotsåè®®
	if method != "HEAD" && !c.IgnoreRobotsTxt {
		if err = c.checkRobots(parsedURL); err != nil {
			return err
		}
	}
     // headers
	if hdr == nil {
		hdr = http.Header{"User-Agent": []string{c.UserAgent}}
	}
	rc, ok := requestData.(io.ReadCloser)
	if !ok && requestData != nil {
		rc = ioutil.NopCloser(requestData)
	}
	// The Go HTTP API ignores "Host" in the headers, preferring the client
	// to use the Host field on Request.
	host := parsedURL.Host
	if hostHeader := hdr.Get("Host"); hostHeader != "" {
		host = hostHeader
	}
    // æ„é€ http.Request
	req := &http.Request{
		Method:     method,
		URL:        parsedURL,
		Proto:      "HTTP/1.1",
		ProtoMajor: 1,
		ProtoMinor: 1,
		Header:     hdr,
		Body:       rc,
		Host:       host,
	}
    // è¯·æ±‚çš„æ•°æ®ï¼ˆrequestDataï¼‰è½¬æ¢æˆio.ReadCloseræ¥å£æ•°æ®
	setRequestBody(req, requestData)
	u = parsedURL.String()
	c.wg.Add(1)
    // å¼‚æ­¥æ–¹å¼
	if c.Async {
		go c.fetch(u, method, depth, requestData, ctx, hdr, req)
		return nil
	}
	return c.fetch(u, method, depth, requestData, ctx, hdr, req)
}
```

ä¸Šé¢å¾ˆå¤§ç¯‡å¹…éƒ½æ˜¯æ£€æŸ¥ï¼Œ ç°åœ¨è¿˜åœ¨ `request`çš„é˜¶æ®µï¼Œ è¿˜æ²¡æœ‰responseï¼Œçœ‹`c.fetch`

fetchå°±æ˜¯collyçš„æ ¸å¿ƒå†…å®¹

```go
func (c *Collector) fetch(u, method string, depth int, requestData io.Reader, ctx *Context, hdr http.Header, req *http.Request) error {
	defer c.wg.Done()
	if ctx == nil {
		ctx = NewContext()
	}
	request := &Request{
		URL:       req.URL,
		Headers:   &req.Header,
		Ctx:       ctx,
		Depth:     depth,
		Method:    method,
		Body:      requestData,
		collector: c, // è¿™é‡Œå°†Collectoræ”¾åˆ°requestä¸­ï¼Œè¿™ä¸ªå¯ä»¥å¯¹è¯·æ±‚ç»§ç»­å¤„ç†
		ID:        atomic.AddUint32(&c.requestCount, 1),
	}
    // å›è°ƒå‡½æ•°å¤„ç† request
	c.handleOnRequest(request)

	if request.abort {
		return nil
	}

	if method == "POST" && req.Header.Get("Content-Type") == "" {
		req.Header.Add("Content-Type", "application/x-www-form-urlencoded")
	}

	if req.Header.Get("Accept") == "" {
		req.Header.Set("Accept", "*/*")
	}

	origURL := req.URL
    // è¿™é‡Œæ˜¯ å»è¯·æ±‚ç½‘ç»œï¼Œ æ˜¯è°ƒç”¨äº† `http.Client.Do`æ–¹æ³•è¯·æ±‚çš„
	response, err := c.backend.Cache(req, c.MaxBodySize, c.CacheDir)
	if proxyURL, ok := req.Context().Value(ProxyURLKey).(string); ok {
		request.ProxyURL = proxyURL
	}
    // å›è°ƒå‡½æ•°ï¼Œå¤„ç†error
	if err := c.handleOnError(response, err, request, ctx); err != nil {
		return err
	}
	if req.URL != origURL {
		request.URL = req.URL
		request.Headers = &req.Header
	}
	atomic.AddUint32(&c.responseCount, 1)
	response.Ctx = ctx
	response.Request = request

	err = response.fixCharset(c.DetectCharset, request.ResponseCharacterEncoding)
	if err != nil {
		return err
	}
    // å›è°ƒå‡½æ•° å¤„ç†Response
	c.handleOnResponse(response)
    
    // å›è°ƒå‡½æ•° HTML
	err = c.handleOnHTML(response)
	if err != nil {
		c.handleOnError(response, err, request, ctx)
	}
    // å›è°ƒå‡½æ•°XML
	err = c.handleOnXML(response)
	if err != nil {
		c.handleOnError(response, err, request, ctx)
	}
    // å›è°ƒå‡½æ•° Scraped
	c.handleOnScraped(response)

	return err
}


```

çœ‹åˆ°äº†ï¼Œ è¿™å°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„æµç¨‹ã€‚ å¥½ï¼Œ æˆ‘ä»¬çœ‹ä¸€ä¸‹å›è°ƒå‡½æ•°åšäº†ä»€ä¹ˆï¼Ÿ
```go
func (c *Collector) handleOnRequest(r *Request) {
	if c.debugger != nil {
		c.debugger.Event(createEvent("request", r.ID, c.ID, map[string]string{
			"url": r.URL.String(),
		}))
	}
	for _, f := range c.requestCallbacks {
		f(r)
	}
}
```
æ ¸å¿ƒå°± `for _, f := range c.requestCallbacks {
f(r)
}`è¿™å¥ï¼Œä¸‹é¢æˆ‘æ¯ä¸ªå›è°ƒå‡½æ•°éƒ½ä»‹ç»ä¸€ä¸‹


### å›è°ƒå‡½æ•°

è¿™é‡Œä»‹ç»æŒ‰ç”Ÿå‘½å‘¨æœŸçš„é¡ºåºæ¥ä»‹ç»


#### 1. OnRequest
 ```go
// OnRequest registers a function. Function will be executed on every
// request made by the Collector
// è¿™é‡Œæ˜¯æ³¨å†Œå›è°ƒå‡½æ•°åˆ° requestCallbacks
func (c *Collector) OnRequest(f RequestCallback) {
	c.lock.Lock()
	if c.requestCallbacks == nil {
		c.requestCallbacks = make([]RequestCallback, 0, 4)
	}
	c.requestCallbacks = append(c.requestCallbacks, f)
	c.lock.Unlock()
}


// åœ¨fetchä¸­è°ƒç”¨æœ€æ—©è°ƒç”¨çš„
func (c *Collector) handleOnRequest(r *Request) {
	if c.debugger != nil {
		c.debugger.Event(createEvent("request", r.ID, c.ID, map[string]string{
			"url": r.URL.String(),
		}))
	}
	for _, f := range c.requestCallbacks {
		f(r)
	}
}
```


#### 2. OnResponse & handleOnResponse
```go
// OnResponse registers a function. Function will be executed on every response
func (c *Collector) OnResponse(f ResponseCallback) {
	c.lock.Lock()
	if c.responseCallbacks == nil {
		c.responseCallbacks = make([]ResponseCallback, 0, 4)
	}
	c.responseCallbacks = append(c.responseCallbacks, f)
	c.lock.Unlock()
}


func (c *Collector) handleOnResponse(r *Response) {
	if c.debugger != nil {
		c.debugger.Event(createEvent("response", r.Request.ID, c.ID, map[string]string{
			"url":    r.Request.URL.String(),
			"status": http.StatusText(r.StatusCode),
		}))
	}
	for _, f := range c.responseCallbacks {
		f(r)
	}
}

```


#### 3. OnHTML & handleOnHTML
```go
// OnHTML registers a function. Function will be executed on every HTML
// element matched by the GoQuery Selector parameter.
// GoQuery Selector is a selector used by https://github.com/PuerkitoBio/goquery
func (c *Collector) OnHTML(goquerySelector string, f HTMLCallback) {
	c.lock.Lock()
	if c.htmlCallbacks == nil {
		c.htmlCallbacks = make([]*htmlCallbackContainer, 0, 4)
	}
	c.htmlCallbacks = append(c.htmlCallbacks, &htmlCallbackContainer{
		Selector: goquerySelector,
		Function: f,
	})
	c.lock.Unlock()
}

// è¿™ä¸ªè§£æhtmlçš„é€»è¾‘æ¯”è¾ƒå¤šä¸€äº›
func (c *Collector) handleOnHTML(resp *Response) error {
	if len(c.htmlCallbacks) == 0 || !strings.Contains(strings.ToLower(resp.Headers.Get("Content-Type")), "html") {
		return nil
	}
	doc, err := goquery.NewDocumentFromReader(bytes.NewBuffer(resp.Body))
	if err != nil {
		return err
	}
	if href, found := doc.Find("base[href]").Attr("href"); found {
		resp.Request.baseURL, _ = url.Parse(href)
	}
	for _, cc := range c.htmlCallbacks {
		i := 0
		doc.Find(cc.Selector).Each(func(_ int, s *goquery.Selection) {
			for _, n := range s.Nodes {
				e := NewHTMLElementFromSelectionNode(resp, s, n, i)
				i++
				if c.debugger != nil {
					c.debugger.Event(createEvent("html", resp.Request.ID, c.ID, map[string]string{
						"selector": cc.Selector,
						"url":      resp.Request.URL.String(),
					}))
				}
				cc.Function(e)
			}
		})
	}
	return nil
}

```

#### 4. OnXML & handleOnXML

```go
// OnXML registers a function. Function will be executed on every XML
// element matched by the xpath Query parameter.
// xpath Query is used by https://github.com/antchfx/xmlquery
func (c *Collector) OnXML(xpathQuery string, f XMLCallback) {
	c.lock.Lock()
	if c.xmlCallbacks == nil {
		c.xmlCallbacks = make([]*xmlCallbackContainer, 0, 4)
	}
	c.xmlCallbacks = append(c.xmlCallbacks, &xmlCallbackContainer{
		Query:    xpathQuery,
		Function: f,
	})
	c.lock.Unlock()
}



func (c *Collector) handleOnXML(resp *Response) error {
	if len(c.xmlCallbacks) == 0 {
		return nil
	}
	contentType := strings.ToLower(resp.Headers.Get("Content-Type"))
	isXMLFile := strings.HasSuffix(strings.ToLower(resp.Request.URL.Path), ".xml") || strings.HasSuffix(strings.ToLower(resp.Request.URL.Path), ".xml.gz")
	if !strings.Contains(contentType, "html") && (!strings.Contains(contentType, "xml") && !isXMLFile) {
		return nil
	}

	if strings.Contains(contentType, "html") {
		doc, err := htmlquery.Parse(bytes.NewBuffer(resp.Body))
		if err != nil {
			return err
		}
		if e := htmlquery.FindOne(doc, "//base"); e != nil {
			for _, a := range e.Attr {
				if a.Key == "href" {
					resp.Request.baseURL, _ = url.Parse(a.Val)
					break
				}
			}
		}

		for _, cc := range c.xmlCallbacks {
			for _, n := range htmlquery.Find(doc, cc.Query) {
				e := NewXMLElementFromHTMLNode(resp, n)
				if c.debugger != nil {
					c.debugger.Event(createEvent("xml", resp.Request.ID, c.ID, map[string]string{
						"selector": cc.Query,
						"url":      resp.Request.URL.String(),
					}))
				}
				cc.Function(e)
			}
		}
	} else if strings.Contains(contentType, "xml") || isXMLFile {
		doc, err := xmlquery.Parse(bytes.NewBuffer(resp.Body))
		if err != nil {
			return err
		}

		for _, cc := range c.xmlCallbacks {
			xmlquery.FindEach(doc, cc.Query, func(i int, n *xmlquery.Node) {
				e := NewXMLElementFromXMLNode(resp, n)
				if c.debugger != nil {
					c.debugger.Event(createEvent("xml", resp.Request.ID, c.ID, map[string]string{
						"selector": cc.Query,
						"url":      resp.Request.URL.String(),
					}))
				}
				cc.Function(e)
			})
		}
	}
	return nil
}



```


#### 5. OnError & handleOnError
è¿™ä¸ªä¼šå¤šæ¬¡è°ƒç”¨ï¼Œ å¦‚æœ `err != nilæƒ…å†µä¸‹è°ƒç”¨æ¯”è¾ƒå¤š`ï¼Œ çˆ¬è™«å¼‚å¸¸çš„æƒ…å†µä¸‹ï¼Œä¼šè°ƒç”¨

```go
// OnError registers a function. Function will be executed if an error
// occurs during the HTTP request.
func (c *Collector) OnError(f ErrorCallback) {
	c.lock.Lock()
	if c.errorCallbacks == nil {
		c.errorCallbacks = make([]ErrorCallback, 0, 4)
	}
	c.errorCallbacks = append(c.errorCallbacks, f)
	c.lock.Unlock()
}


func (c *Collector) handleOnError(response *Response, err error, request *Request, ctx *Context) error {
	if err == nil && (c.ParseHTTPErrorResponse || response.StatusCode < 203) {
		return nil
	}
	if err == nil && response.StatusCode >= 203 {
		err = errors.New(http.StatusText(response.StatusCode))
	}
	if response == nil {
		response = &Response{
			Request: request,
			Ctx:     ctx,
		}
	}
	if c.debugger != nil {
		c.debugger.Event(createEvent("error", request.ID, c.ID, map[string]string{
			"url":    request.URL.String(),
			"status": http.StatusText(response.StatusCode),
		}))
	}
	if response.Request == nil {
		response.Request = request
	}
	if response.Ctx == nil {
		response.Ctx = request.Ctx
	}
	for _, f := range c.errorCallbacks {
		f(response, err)
	}
	return err
}
```

#### 6. OnScraped & handleOnScraped
æœ€åä¸€æ­¥çš„å›è°ƒå‡½æ•°å¤„ç†
```go
// OnScraped registers a function. Function will be executed after
// OnHTML, as a final part of the scraping.
func (c *Collector) OnScraped(f ScrapedCallback) {
	c.lock.Lock()
	if c.scrapedCallbacks == nil {
		c.scrapedCallbacks = make([]ScrapedCallback, 0, 4)
	}
	c.scrapedCallbacks = append(c.scrapedCallbacks, f)
	c.lock.Unlock()
}

func (c *Collector) handleOnScraped(r *Response) {
	if c.debugger != nil {
		c.debugger.Event(createEvent("scraped", r.Request.ID, c.ID, map[string]string{
			"url": r.Request.URL.String(),
		}))
	}
	for _, f := range c.scrapedCallbacks {
		f(r)
	}
}
```
æ³¨å†Œå›è°ƒå‡½æ•°çš„methodè¿˜æœ‰å‡ ä¸ªæ²¡æœ‰åˆ—å‡ºæ¥ï¼Œæ„Ÿå…´è¶£çš„ï¼Œè‡ªå·±çœ‹ä¸€ä¸‹ï¼Œ

ä¸Šé¢ä»‹ç»å®Œäº†ï¼Œ å†å›å¤´çœ‹ğŸŒ°
```go
	// On every a element which has href attribute call callback
	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Attr("href")
		// Print link
		fmt.Printf("Link found: %q -> %s\n", e.Text, link)
		// Visit link found on page
		// Only those links are visited which are in AllowedDomains
		c.Visit(e.Request.AbsoluteURL(link))
	})

	// Before making a request print "Visiting ..."
	c.OnRequest(func(r *colly.Request) {
		fmt.Println("Visiting", r.URL.String())
	})
```
ä¸€èˆ¬æ–‡æ¡£è§£ææ”¾åœ¨html, xml ä¸­



### é¡µé¢è·³è½¬çˆ¬å–
ä¸€èˆ¬å¤„ç†å°±2ç§ï¼Œä¸€ç§æ˜¯ç›¸åŒé€»è¾‘çš„é¡µé¢ï¼Œæ¯”å¦‚`ä¸‹ä¸€é¡µ`ï¼Œå¦ä¸€ç§ï¼Œå°±æ˜¯ä¸åŒé€»è¾‘çš„ï¼Œæ¯”å¦‚`å­é¡µé¢`
1.  åœ¨`html`,`xml`ï¼Œè§£æå‡ºæ¥ä»¥åï¼Œæ„å»ºæ–°çš„è¯·æ±‚ï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹ï¼Œç›¸åŒé¡µé¢
 ```go
	// On every a element which has href attribute call callback
	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		// If attribute class is this long string return from callback
		// As this a is irrelevant
		if e.Attr("class") == "Button_1qxkboh-o_O-primary_cv02ee-o_O-md_28awn8-o_O-primaryLink_109aggg" {
			return
		}
		link := e.Attr("href")
		// If link start with browse or includes either signup or login return from callback
		if !strings.HasPrefix(link, "/browse") || strings.Index(link, "=signup") > -1 || strings.Index(link, "=login") > -1 {
			return
		}
		// start scaping the page under the link found
		e.Request.Visit(link)
	})

```
ä¸Šé¢æ˜¯ HTMLçš„å›è°ƒå‡½æ•°ï¼Œè§£æé¡µé¢ï¼Œè·å–äº†`url`,ä½¿ç”¨ `e.Request.Visit(link)`, å…¶å®å°±æ˜¯ `e.Request.collector.Visit(link)`
æˆ‘è§£é‡Šä¸€ä¸‹
```go
func (c *Collector) fetch(u, method string, depth int, requestData io.Reader, ctx *Context, hdr http.Header, req *http.Request) error {
	defer c.wg.Done()
	if ctx == nil {
		ctx = NewContext()
	}
	request := &Request{
		URL:       req.URL,
		Headers:   &req.Header,
		Ctx:       ctx,
		Depth:     depth,
		Method:    method,
		Body:      requestData,
		collector: c, // è¿™ä¸ªä¸Šé¢æœ‰ä»‹ç»
		ID:        atomic.AddUint32(&c.requestCount, 1),
	}
    ....
    }}


// Visit continues Collector's collecting job by creating a
// request and preserves the Context of the previous request.
// Visit also calls the previously provided callbacks
func (r *Request) Visit(URL string) error {
	return r.collector.scrape(r.AbsoluteURL(URL), "GET", r.Depth+1, nil, r.Ctx, nil, true)
}
```
è¿™ç§æ–¹æ³•åœ¨å®é™…å¼€å‘ä¸­ç»å¸¸ä¼šç”¨åˆ°ã€‚


2. å­é¡µé¢çš„å¤„ç†é€»è¾‘
   collyä¸­ä¸»è¦æ˜¯ä»¥`Collector`ä¸ºä¸­å¿ƒï¼Œ ç„¶åå„ç§å›è°ƒå‡½æ•°è¿›è¡Œå¤„ç†ï¼Œå­é¡µé¢éœ€è¦ä¸åŒçš„å›è°ƒå‡½æ•°ï¼Œæ‰€ä»¥å°±éœ€è¦æ–°çš„ `Collector`

```go
	// Instantiate default collector
	c := colly.NewCollector(
		// Visit only domains: coursera.org, www.coursera.org
		colly.AllowedDomains("coursera.org", "www.coursera.org"),

		// Cache responses to prevent multiple download of pages
		// even if the collector is restarted
		colly.CacheDir("./coursera_cache"),
	)

	// Create another collector to scrape course details
	detailCollector := c.Clone()

	// Before making a request print "Visiting ..."
	c.OnRequest(func(r *colly.Request) {
		log.Println("visiting", r.URL.String())
	})

	// On every a HTML element which has name attribute call callback
	c.OnHTML(`a[name]`, func(e *colly.HTMLElement) {
		// Activate detailCollector if the link contains "coursera.org/learn"
		courseURL := e.Request.AbsoluteURL(e.Attr("href"))
		if strings.Index(courseURL, "coursera.org/learn") != -1 {
           // å­é¡µé¢æˆ–å…¶ä»–é¡µé¢
			detailCollector.Visit(courseURL)
		}
	})
```


### æŒä¹…åŒ–
`Collector`å¯¹è±¡æœ‰ä¸€ä¸ªå±æ€§ `store             storage.Storage`æ˜¯å­˜å‚¨çš„ï¼Œè¿™ä¸ªæ˜¯å°†æ•°æ®ç›´æ¥å­˜å‚¨ä¸‹æ¥ï¼Œæ²¡æœ‰æ¸…æ´—ã€‚
æ¯”å¦‚ï¼Œ æˆ‘éœ€è¦å°†æ•°æ®æŒä¹…åŒ–åˆ°æ•°æ®åº“ä¸­ï¼Œå…¶å®å¾ˆç®€å•ï¼Œ åœ¨å›è°ƒå‡½æ•°ä¸­å¤„ç†ã€‚

ç»™ä¸ªä¾‹å­
```go
	c.OnHTML("#currencies-all tbody tr", func(e *colly.HTMLElement) {
		mysql.WriteObjectStrings([]string{
			e.ChildText(".currency-name-container"),
			e.ChildText(".col-symbol"),
			e.ChildAttr("a.price", "data-usd"),
			e.ChildAttr("a.volume", "data-usd"),
			e.ChildAttr(".market-cap", "data-usd"),
			e.ChildAttr(".percent-change[data-timespan=\"1h\"]", "data-percentusd"),
			e.ChildAttr(".percent-change[data-timespan=\"24h\"]", "data-percentusd"),
			e.ChildAttr(".percent-change[data-timespan=\"7d\"]", "data-percentusd"),
		})
	})
```


### æ€»ç»“
å¥½äº†ï¼Œä»‹ç»å®Œäº†ï¼Œæˆ‘æ²¡æœ‰ä»‹ç»å¦‚ä½•ä½¿ç”¨ï¼Œæˆ‘è‡ªå·±ä¹Ÿæ²¡æœ‰å†™ä»»ä½•çš„ä»£ç ï¼Œ æˆ‘åªæƒ³åˆ†äº«ç»™ä½ è¿™ç§è½¯ä»¶æ¶æ„çš„ç‰¹ç‚¹ä»¥åŠè®¾è®¡æ¨¡å¼ï¼Œ å¸Œæœ›ä½ å¯ä»¥å€Ÿé‰´åº”ç”¨åˆ°å·¥ä½œä¸­ï¼Œä¸€èˆ¬å†™æ¡†æ¶éƒ½æ˜¯é‡‡ç”¨è¿™ç§æ€ç»´ã€‚  
ä¸‹é¢è¿™å¼ å›¾å¾ˆå½¢è±¡ï¼Œçˆ¬è™«æ¡†æ¶å°±è¿™äº›ä¸œè¥¿ã€‚
![é€šç”¨çˆ¬è™«æ¡†æ¶æ¶æ„](https://raw.githubusercontent.com/jacksonyoudi/images/main/uPic/BqC7ix.jpg)